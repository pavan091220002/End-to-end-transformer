{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Transformer Language Model Implementation\n",
    "\n",
    "### Introduction: From Tokens to Transformers for Text Generation\n",
    "\n",
    "**Recap: The Role of Tokenization**\n",
    "\n",
    "As explored previously with techniques like Byte Pair Encoding (BPE), the first step in processing text for machine learning models is **tokenization**. This breaks raw text into manageable units (tokens). For simplicity in this notebook, we will perform basic **character-level tokenization**, where each unique character in our text becomes a distinct token. These tokens are then mapped to unique numerical IDs. Our main focus here is on building the model that learns from sequences of these IDs: a Language Model.\n",
    "\n",
    "**Goal: Building a Generative Transformer**\n",
    "\n",
    "Our objective is to construct a basic Language Model using the **Transformer architecture**. Specifically, we will build a **Decoder-only Transformer**, similar in style to models like GPT. This model learns to predict the next token (character, in our case) in a sequence, given the preceding tokens. By repeatedly predicting the next token and feeding it back into the input, the model can generate new sequences of text, character by character.\n",
    "\n",
    "**The Transformer Architecture: Key Concepts**\n",
    "\n",
    "The Transformer, introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), uses **attention mechanisms** to weigh the importance of different input tokens when processing a particular token, eliminating the need for recurrence used in RNNs/LSTMs. Key components we will implement inline:\n",
    "\n",
    "1.  **Input Embeddings:** Converting numerical token (character) IDs into dense vector representations.\n",
    "2.  **Positional Encoding:** Adding information about the position of tokens in the sequence.\n",
    "3.  **Multi-Head Self-Attention (Masked):** Allowing the model to attend to previous tokens in the sequence to predict the next one. The mask prevents attending to future tokens.\n",
    "4.  **Add & Norm Layers:** Residual connections followed by Layer Normalization for stable training.\n",
    "5.  **Position-wise Feed-Forward Networks:** Applying a non-linear transformation independently to each token representation.\n",
    "6.  **Decoder Blocks:** Stacking these components multiple times.\n",
    "7.  **Final Linear Layer & Softmax:** Mapping final representations back to vocabulary scores (logits) and then probabilities.\n",
    "\n",
    "**This Notebook's Approach:**\n",
    "\n",
    "We will implement this architecture step-by-step, inline, without defining functions or classes. Each conceptual part will be broken into minimal code blocks with extremely detailed explanations and mathematical formulations (using LaTeX). We will use a small text dataset and model configuration for transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup - Libraries, Corpus, Tokenization, Hyperparameters\n",
    "\n",
    "**Goal:** Prepare the environment by importing PyTorch, defining the text corpus, performing character-level tokenization, and setting the model's configuration (hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.1: Import Libraries\n",
    "\n",
    "**Explanation:** We need `torch` for tensor operations, neural network components (`nn`), activation functions (`F`), optimizers (`optim`), and `math` for calculations like square root in attention scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import os\n",
    "\n",
    "# For reproducibility (optional, but good practice)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.2: Define the Training Corpus\n",
    "\n",
    "**Explanation:** We'll use the same excerpt from \"Alice's Adventures in Wonderland\" as our training data. This provides a small but somewhat realistic text source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus defined (length: 593 characters).\n"
     ]
    }
   ],
   "source": [
    "# Define the raw text corpus for training\n",
    "corpus_raw = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the\n",
    "bank, and of having nothing to do: once or twice she had peeped into the\n",
    "book her sister was reading, but it had no pictures or conversations in\n",
    "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
    "conversation?'\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "hot day made her feel very sleepy and stupid), whether the pleasure\n",
    "of making a daisy-chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Training corpus defined (length: {len(corpus_raw)} characters).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.3: Character-Level Tokenization\n",
    "\n",
    "**Explanation:** We create a vocabulary consisting of all unique characters present in the corpus. We then create mappings: one from each character to a unique integer ID (`char_to_int`) and its inverse (`int_to_char`). The size of this unique character set determines our `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created character vocabulary of size: 36\n",
      "Vocabulary: \n",
      " '(),-.:?ARSWabcdefghiklmnoprstuvwy\n"
     ]
    }
   ],
   "source": [
    "# Find all unique characters in the raw corpus\n",
    "chars = sorted(list(set(corpus_raw)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create character-to-integer mapping (encoding)\n",
    "char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# Create integer-to-character mapping (decoding)\n",
    "int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "print(f\"Created character vocabulary of size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "# print(f\"Char-to-Int mapping: {char_to_int}\") # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.4: Encode the Corpus\n",
    "\n",
    "**Explanation:** Convert the entire raw text corpus into a sequence of integer token IDs using the `char_to_int` mapping. This numerical sequence is the actual input the model will process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded corpus into a tensor of shape: torch.Size([593])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire corpus into a list of integer IDs\n",
    "encoded_corpus = [char_to_int[ch] for ch in corpus_raw]\n",
    "\n",
    "# Convert the list into a PyTorch tensor\n",
    "full_data_sequence = torch.tensor(encoded_corpus, dtype=torch.long)\n",
    "\n",
    "print(f\"Encoded corpus into a tensor of shape: {full_data_sequence.shape}\")\n",
    "# print(f\"First 100 encoded token IDs: {full_data_sequence[:100].tolist()}\") # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.5: Define Hyperparameters\n",
    "\n",
    "**Explanation:** Set the configuration for our model and training. We use the `vocab_size` determined from the corpus. Other values are kept small for demonstration.\n",
    "*   `d_model`: Dimension of embeddings and internal representations.\n",
    "*   `n_heads`: Number of parallel attention calculations.\n",
    "*   `n_layers`: Number of decoder blocks.\n",
    "*   `d_ff`: Hidden dimension in the feed-forward networks.\n",
    "*   `block_size`: Maximum sequence length processed at once.\n",
    "*   `learning_rate`, `batch_size`, `epochs`: Training control parameters.\n",
    "*   `device`: Use GPU ('cuda') if available, otherwise CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters defined:\n",
      "  vocab_size: 36\n",
      "  d_model: 64\n",
      "  n_heads: 4\n",
      "  d_k (dim per head): 16\n",
      "  n_layers: 3\n",
      "  d_ff: 256\n",
      "  block_size: 32\n",
      "  learning_rate: 0.0003\n",
      "  batch_size: 16\n",
      "  epochs: 5000\n",
      "  Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define Model Hyperparameters (using calculated vocab_size)\n",
    "# vocab_size = vocab_size # Already defined from data\n",
    "d_model = 64         # Embedding dimension (increased slightly for characters)\n",
    "n_heads = 4          # Number of attention heads\n",
    "n_layers = 3         # Number of Transformer blocks\n",
    "d_ff = d_model * 4   # Dimension of the feed-forward inner layer\n",
    "block_size = 32      # Maximum context length (sequence length)\n",
    "# dropout_rate = 0.1 # Omitting dropout layers for inline simplicity\n",
    "\n",
    "# Define Training Hyperparameters\n",
    "learning_rate = 3e-4 # Slightly smaller LR often better for AdamW\n",
    "batch_size = 16      # Process 16 sequences per step\n",
    "epochs = 5000        # Increase epochs for character-level model to see learning\n",
    "eval_interval = 500 # How often to print loss\n",
    "\n",
    "# Device Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure d_model is divisible by n_heads\n",
    "assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "d_k = d_model // n_heads # Dimension of keys/queries/values per head\n",
    "\n",
    "print(f\"Hyperparameters defined:\")\n",
    "print(f\"  vocab_size: {vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  d_k (dim per head): {d_k}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  block_size: {block_size}\")\n",
    "print(f\"  learning_rate: {learning_rate}\")\n",
    "print(f\"  batch_size: {batch_size}\")\n",
    "print(f\"  epochs: {epochs}\")\n",
    "print(f\"  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation for Training\n",
    "\n",
    "**Goal:** Structure the encoded data (`full_data_sequence`) into input (`x`) and target (`y`) pairs suitable for training the next-token prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Create Input (x) and Target (y) Pairs\n",
    "\n",
    "**Explanation:** The model needs to learn `P(token_i | token_0, ..., token_{i-1})`. We achieve this by creating sequences of length `block_size`. For each input sequence `x` taken from `data[i : i+block_size]`, the corresponding target sequence `y` is the next token for each position in `x`, i.e., `data[i+1 : i+block_size+1]`. We extract all possible overlapping sequences from our encoded corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 561 overlapping input/target sequence pairs.\n",
      "Shape of train_x: torch.Size([561, 32])\n",
      "Shape of train_y: torch.Size([561, 32])\n"
     ]
    }
   ],
   "source": [
    "# Create lists to hold all possible input (x) and target (y) sequences of length block_size\n",
    "all_x = []\n",
    "all_y = []\n",
    "\n",
    "# Iterate through the encoded corpus tensor to extract overlapping sequences\n",
    "# We need to stop early enough so that we can always get a target sequence of the same length\n",
    "num_total_tokens = len(full_data_sequence)\n",
    "for i in range(num_total_tokens - block_size):\n",
    "    # Extract the input sequence chunk of length block_size\n",
    "    x_chunk = full_data_sequence[i : i + block_size]\n",
    "    # Extract the target sequence chunk (shifted one position to the right)\n",
    "    y_chunk = full_data_sequence[i + 1 : i + block_size + 1]\n",
    "    \n",
    "    # Append the chunks to our lists\n",
    "    all_x.append(x_chunk)\n",
    "    all_y.append(y_chunk)\n",
    "\n",
    "# Stack the lists of tensors into single large tensors\n",
    "# train_x will have shape (num_sequences, block_size)\n",
    "# train_y will have shape (num_sequences, block_size)\n",
    "train_x = torch.stack(all_x)\n",
    "train_y = torch.stack(all_y)\n",
    "\n",
    "num_sequences_available = train_x.shape[0]\n",
    "print(f\"Created {num_sequences_available} overlapping input/target sequence pairs.\")\n",
    "print(f\"Shape of train_x: {train_x.shape}\")\n",
    "print(f\"Shape of train_y: {train_y.shape}\")\n",
    "\n",
    "# Optional: Display a sample input/target pair and decode it\n",
    "# sample_idx = 0\n",
    "# sample_x_ids = train_x[sample_idx].tolist()\n",
    "# sample_y_ids = train_y[sample_idx].tolist()\n",
    "# sample_x_chars = ''.join([int_to_char[id] for id in sample_x_ids])\n",
    "# sample_y_chars = ''.join([int_to_char[id] for id in sample_y_ids])\n",
    "# print(f\"\\nSample Input x[{sample_idx}] IDs:  {sample_x_ids}\")\n",
    "# print(f\"Sample Target y[{sample_idx}] IDs: {sample_y_ids}\")\n",
    "# print(f\"Sample Input x[{sample_idx}] Chars: '{sample_x_chars}'\")\n",
    "# print(f\"Sample Target y[{sample_idx}] Chars:'{sample_y_chars}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Batching Strategy (Simplified: Random Sampling)\n",
    "\n",
    "**Explanation:** Instead of implementing a complex data loader, for each training step we will simply select `batch_size` random indices from our available sequences (`0` to `num_sequences_available - 1`). We then use these indices to grab the corresponding input (`xb`) and target (`yb`) sequences from `train_x` and `train_y`. This simulates drawing random batches from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready for training. Will sample batches of size 16 randomly.\n"
     ]
    }
   ],
   "source": [
    "# Check if we have enough sequences for the desired batch size\n",
    "if num_sequences_available < batch_size:\n",
    "    print(f\"Warning: Number of sequences ({num_sequences_available}) is less than batch size ({batch_size}). Adjusting batch size.\")\n",
    "    batch_size = num_sequences_available\n",
    "\n",
    "print(f\"Data ready for training. Will sample batches of size {batch_size} randomly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Model Component Initialization\n",
    "\n",
    "**Goal:** Initialize the learnable parameters for all layers of our Transformer model. Each layer is created as an instance of a `torch.nn` module and moved to the target `device`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Token Embedding Layer\n",
    "\n",
    "**Explanation:** Maps integer token IDs (character IDs in our case) to dense vectors. Input `(B, T)` -> Output `(B, T, C)` where `B`=batch, `T`=time/sequence length, `C`=`d_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Token Embedding Layer (Vocab: 36, Dim: 64). Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize the token embedding table (lookup table)\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "\n",
    "print(f\"Initialized Token Embedding Layer (Vocab: {vocab_size}, Dim: {d_model}). Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Positional Encoding Matrix\n",
    "\n",
    "**Explanation:** Creates fixed (non-learned) vectors that encode position information using sine and cosine functions of varying frequencies. This matrix `(1, block_size, d_model)` will be added to the token embeddings.\n",
    "Formulas:\n",
    "$$ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i / d_{\\text{model}}}}\\right) $$ \n",
    "$$ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i / d_{\\text{model}}}}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.2: Creating Positional Encoding matrix...\n",
      "  Positional Encoding matrix created with shape: torch.Size([1, 32, 64]). Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Precompute the Sinusoidal Positional Encoding matrix\n",
    "print(\"Step 2.2: Creating Positional Encoding matrix...\")\n",
    "\n",
    "# Matrix to store encodings: Shape (block_size, d_model)\n",
    "positional_encoding = torch.zeros(block_size, d_model, device=device)\n",
    "\n",
    "# Position indices (0 to block_size-1): Shape (block_size, 1)\n",
    "position = torch.arange(0, block_size, dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "# Dimension indices (0, 2, 4, ...): Shape (d_model/2)\n",
    "div_term_indices = torch.arange(0, d_model, 2, dtype=torch.float, device=device)\n",
    "# Denominator term: 1 / (10000^(2i / d_model))\n",
    "div_term = torch.exp(div_term_indices * (-math.log(10000.0) / d_model))\n",
    "\n",
    "# Calculate sine for even dimensions\n",
    "positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "# Calculate cosine for odd dimensions\n",
    "positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "# Add batch dimension: Shape (1, block_size, d_model)\n",
    "positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "print(f\"  Positional Encoding matrix created with shape: {positional_encoding.shape}. Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: Transformer Block Components Initialization\n",
    "\n",
    "**Explanation:** Initialize all components needed for the `n_layers` decoder blocks. We store them in Python lists, where the index corresponds to the layer number (0 to `n_layers-1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.3: Initializing components for 3 Transformer layers...\n",
      "  Initialized components for Layer 1/3.\n",
      "  Initialized components for Layer 2/3.\n",
      "  Initialized components for Layer 3/3.\n",
      "Finished initializing components for 3 layers.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Step 2.3: Initializing components for {n_layers} Transformer layers...\")\n",
    "\n",
    "# Lists to store layers for each Transformer block\n",
    "layer_norms_1 = []      # LayerNorm after MHA\n",
    "layer_norms_2 = []      # LayerNorm after FFN\n",
    "mha_qkv_linears = []    # Combined Linear layer for Q, K, V projections\n",
    "mha_output_linears = [] # Output Linear layer for MHA\n",
    "ffn_linear_1 = []       # First linear layer in FFN\n",
    "ffn_linear_2 = []       # Second linear layer in FFN\n",
    "\n",
    "# Loop through the number of layers\n",
    "for i in range(n_layers):\n",
    "    # Layer Normalization 1 (for post-MHA residual)\n",
    "    ln1 = nn.LayerNorm(d_model).to(device)\n",
    "    layer_norms_1.append(ln1)\n",
    "\n",
    "    # Multi-Head Attention: Combined QKV projection layer\n",
    "    qkv_linear = nn.Linear(d_model, 3 * d_model, bias=False).to(device) # Often bias=False here\n",
    "    mha_qkv_linears.append(qkv_linear)\n",
    "\n",
    "    # Multi-Head Attention: Output projection layer\n",
    "    output_linear = nn.Linear(d_model, d_model).to(device)\n",
    "    mha_output_linears.append(output_linear)\n",
    "\n",
    "    # Layer Normalization 2 (for post-FFN residual)\n",
    "    ln2 = nn.LayerNorm(d_model).to(device)\n",
    "    layer_norms_2.append(ln2)\n",
    "    \n",
    "    # Position-wise Feed-Forward Network: First linear layer\n",
    "    lin1 = nn.Linear(d_model, d_ff).to(device)\n",
    "    ffn_linear_1.append(lin1)\n",
    "    \n",
    "    # Position-wise Feed-Forward Network: Second linear layer\n",
    "    lin2 = nn.Linear(d_ff, d_model).to(device)\n",
    "    ffn_linear_2.append(lin2)\n",
    "    \n",
    "    print(f\"  Initialized components for Layer {i+1}/{n_layers}.\")\n",
    "\n",
    "print(f\"Finished initializing components for {n_layers} layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4: Final Layers Initialization\n",
    "\n",
    "**Explanation:** Initialize the final Layer Normalization applied after the last block and the final Linear layer that maps the Transformer's output back to vocabulary logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.4: Initializing final LayerNorm and Output layers...\n",
      "  Initialized Final LayerNorm. Device: cuda\n",
      "  Initialized Output Linear Layer (to vocab size 36). Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2.4: Initializing final LayerNorm and Output layers...\")\n",
    "\n",
    "# Final Layer Normalization\n",
    "final_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "print(f\"  Initialized Final LayerNorm. Device: {device}\")\n",
    "\n",
    "# Final Linear Layer (language modeling head)\n",
    "output_linear_layer = nn.Linear(d_model, vocab_size).to(device)\n",
    "print(f\"  Initialized Output Linear Layer (to vocab size {vocab_size}). Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Defining the Forward Pass (Inline - Conceptual Blocks)\n",
    "\n",
    "**Goal:** Detail the sequence of operations constituting the Transformer's forward pass. These conceptual blocks will be executed directly within the training and generation loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Input Embedding + Positional Encoding\n",
    "\n",
    "**Explanation:** Convert input token IDs `(B, T)` to embeddings `(B, T, C)` and add positional information. Output `x` has shape `(B, T, C)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual Step 3.1 defined (Embedding + Positional Encoding). Executed in loops.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conceptual Step 3.1 defined (Embedding + Positional Encoding). Executed in loops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Transformer Blocks Loop (Conceptual)\n",
    "\n",
    "**Explanation:** Outline the operations inside the loop that iterates `n_layers` times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2.1: Masked Multi-Head Self-Attention (Conceptual)\n",
    "\n",
    "**Explanation:** Allows each token to attend to previous tokens (including itself). \n",
    "1. Project input `x` `(B, T, C)` to Q, K, V `(B, n_heads, T, d_k)`.\n",
    "2. Calculate scaled dot-product attention scores: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$ where $M$ is the causal mask (setting upper triangle to $-\\infty$).\n",
    "3. Concatenate heads and project back to `(B, T, C)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual Step 3.2.1 defined (Multi-Head Attention). Executed in layer loop.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conceptual Step 3.2.1 defined (Multi-Head Attention). Executed in layer loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2.2: Add & Norm 1 (Post-Attention) (Conceptual)\n",
    "\n",
    "**Explanation:** Residual connection (`x + AttentionOutput`) followed by Layer Normalization. $$ \\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta $$ where $\\mu, \\sigma^2$ are mean/variance over the feature dimension $C$, and $\\gamma, \\beta$ are learnable scale/shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual Step 3.2.2 defined (Add & Norm 1). Executed in layer loop.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conceptual Step 3.2.2 defined (Add & Norm 1). Executed in layer loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2.3: Position-wise Feed-Forward Network (FFN) (Conceptual)\n",
    "\n",
    "**Explanation:** Two linear transformations with a non-linear activation (ReLU) in between, applied independently at each position `t`. $$ \\text{FFN}(x) = \\text{Linear}_2(\\text{ReLU}(\\text{Linear}_1(x))) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual Step 3.2.3 defined (Feed-Forward Network). Executed in layer loop.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conceptual Step 3.2.3 defined (Feed-Forward Network). Executed in layer loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2.4: Add & Norm 2 (Post-FFN) (Conceptual)\n",
    "\n",
    "**Explanation:** Second residual connection (`Norm1Output + FFNOutput`) followed by Layer Normalization. The output of this becomes the input `x` for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual Step 3.2.4 defined (Add & Norm 2). Executed in layer loop.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conceptual Step 3.2.4 defined (Add & Norm 2). Executed in layer loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Final Layers (Conceptual)\n",
    "\n",
    "**Explanation:** Apply final LayerNorm to the output of the last block, then project to vocabulary logits `(B, T, V)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual Step 3.3 defined (Final Layers). Executed after layer loop.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conceptual Step 3.3 defined (Final Layers). Executed after layer loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model (Inline Loop)\n",
    "\n",
    "**Goal:** Iteratively adjust model parameters to minimize the prediction error (loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Define Loss Function\n",
    "\n",
    "**Explanation:** Use Cross-Entropy Loss, suitable for multi-class classification (predicting the next character ID). It requires logits `(N, V)` and targets `(N)`. We'll reshape `(B, T, V)` -> `(B*T, V)` and `(B, T)` -> `(B*T)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4.1: Loss function defined: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Step 4.1: Loss function defined: {type(criterion).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Define Optimizer\n",
    "\n",
    "**Explanation:** Use AdamW optimizer. Gather *all* learnable parameters from the initialized layers into a single list for the optimizer to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4.2: Optimizer defined: AdamW\n",
      "  Managing 38 parameter groups/tensors.\n"
     ]
    }
   ],
   "source": [
    "# Gather all model parameters requiring gradients\n",
    "all_model_parameters = list(token_embedding_table.parameters())\n",
    "for i in range(n_layers):\n",
    "    all_model_parameters.extend(list(layer_norms_1[i].parameters()))\n",
    "    all_model_parameters.extend(list(mha_qkv_linears[i].parameters()))\n",
    "    all_model_parameters.extend(list(mha_output_linears[i].parameters()))\n",
    "    all_model_parameters.extend(list(layer_norms_2[i].parameters()))\n",
    "    all_model_parameters.extend(list(ffn_linear_1[i].parameters()))\n",
    "    all_model_parameters.extend(list(ffn_linear_2[i].parameters()))\n",
    "all_model_parameters.extend(list(final_layer_norm.parameters()))\n",
    "all_model_parameters.extend(list(output_linear_layer.parameters()))\n",
    "\n",
    "# Define the AdamW optimizer\n",
    "optimizer = optim.AdamW(all_model_parameters, lr=learning_rate)\n",
    "\n",
    "print(f\"Step 4.2: Optimizer defined: {type(optimizer).__name__}\")\n",
    "print(f\"  Managing {len(all_model_parameters)} parameter groups/tensors.\")\n",
    "\n",
    "# Create the lower triangular mask for self-attention ONCE, outside the loop\n",
    "# Shape: (1, 1, block_size, block_size)\n",
    "causal_mask = torch.tril(torch.ones(block_size, block_size, device=device)).view(1, 1, block_size, block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: The Training Loop\n",
    "\n",
    "**Explanation:** Iterate for `epochs`. In each step: select batch, perform forward pass (executing conceptual steps 3.1-3.3), calculate loss, zero gradients, backpropagate, update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4.3: Starting Training Loop for 5000 epochs...\n",
      "  Epoch 1/5000, Loss: 3.6902\n",
      "  Epoch 501/5000, Loss: 0.4272\n",
      "  Epoch 1001/5000, Loss: 0.1480\n",
      "  Epoch 1501/5000, Loss: 0.1461\n",
      "  Epoch 2001/5000, Loss: 0.1226\n",
      "  Epoch 2501/5000, Loss: 0.1281\n",
      "  Epoch 3001/5000, Loss: 0.1337\n",
      "  Epoch 3501/5000, Loss: 0.1288\n",
      "  Epoch 4001/5000, Loss: 0.1178\n",
      "  Epoch 4501/5000, Loss: 0.1292\n",
      "  Epoch 5000/5000, Loss: 0.1053\n",
      "--- Training Loop Completed ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStep 4.3: Starting Training Loop for {epochs} epochs...\")\n",
    "\n",
    "# List to track losses\n",
    "losses = []\n",
    "\n",
    "# Set layers to training mode (e.g., for potential dropout, though omitted here)\n",
    "# This doesn't really do anything without dropout/batchnorm, but good practice\n",
    "for i in range(n_layers):\n",
    "    layer_norms_1[i].train()\n",
    "    mha_qkv_linears[i].train()\n",
    "    mha_output_linears[i].train()\n",
    "    layer_norms_2[i].train()\n",
    "    ffn_linear_1[i].train()\n",
    "    ffn_linear_2[i].train()\n",
    "final_layer_norm.train()\n",
    "output_linear_layer.train()\n",
    "token_embedding_table.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # --- 1. Batch Selection --- \n",
    "    indices = torch.randint(0, num_sequences_available, (batch_size,))\n",
    "    xb = train_x[indices].to(device) # Input batch shape: (B, T)\n",
    "    yb = train_y[indices].to(device) # Target batch shape: (B, T)\n",
    "    \n",
    "    # --- 2. Forward Pass (Inline execution) --- \n",
    "    B, T = xb.shape # B = batch_size, T = block_size\n",
    "    C = d_model     # Embedding dimension\n",
    "    \n",
    "    # Step 3.1: Embedding + Positional Encoding\n",
    "    token_embed = token_embedding_table(xb) # (B, T, C)\n",
    "    pos_enc_slice = positional_encoding[:, :T, :] # (1, T, C)\n",
    "    x = token_embed + pos_enc_slice # (B, T, C)\n",
    "    \n",
    "    # Step 3.2: Transformer Blocks\n",
    "    for i in range(n_layers):\n",
    "        # Input to this block\n",
    "        x_input_block = x \n",
    "        \n",
    "        # --- MHA --- \n",
    "        # Apply LayerNorm *before* MHA (Pre-LN variant - common)\n",
    "        x_ln1 = layer_norms_1[i](x_input_block)\n",
    "        # QKV projection\n",
    "        qkv = mha_qkv_linears[i](x_ln1) # (B, T, 3*C)\n",
    "        # Split heads\n",
    "        qkv = qkv.view(B, T, n_heads, 3 * d_k).permute(0, 2, 1, 3) # (B, n_heads, T, 3*d_k)\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # (B, n_heads, T, d_k)\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * (d_k ** -0.5) # (B, n_heads, T, T)\n",
    "        # Apply Causal Mask (use the pre-computed mask sliced to T)\n",
    "        attn_scores_masked = attn_scores.masked_fill(causal_mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(attn_scores_masked, dim=-1) # (B, n_heads, T, T)\n",
    "        # Attention output\n",
    "        attn_output = attention_weights @ v # (B, n_heads, T, d_k)\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, T, C) # (B, T, C)\n",
    "        # Output projection\n",
    "        mha_result = mha_output_linears[i](attn_output) # (B, T, C)\n",
    "        # Add & Norm 1 (Residual connection adds output to original input)\n",
    "        x = x_input_block + mha_result # Residual connection 1\n",
    "        # Note: We moved LN1 to *before* MHA (Pre-LN)\n",
    "        \n",
    "        # --- FFN --- \n",
    "        # Input to FFN\n",
    "        x_input_ffn = x \n",
    "        # Apply LayerNorm *before* FFN (Pre-LN variant)\n",
    "        x_ln2 = layer_norms_2[i](x_input_ffn)\n",
    "        # FFN layers\n",
    "        ffn_hidden = ffn_linear_1[i](x_ln2) # (B, T, d_ff)\n",
    "        ffn_activated = F.relu(ffn_hidden)\n",
    "        ffn_output = ffn_linear_2[i](ffn_activated) # (B, T, C)\n",
    "        # Add & Norm 2 (Residual connection adds output to FFN input)\n",
    "        x = x_input_ffn + ffn_output # Residual connection 2\n",
    "        # Note: We moved LN2 to *before* FFN (Pre-LN)\n",
    "        # Output 'x' of this block becomes input 'x_input_block' for the next block\n",
    "        \n",
    "    # Step 3.3: Final Layers (After loop)\n",
    "    # Apply final LayerNorm (Pre-LN style, applied before final projection)\n",
    "    final_norm_output = final_layer_norm(x) # (B, T, C)\n",
    "    logits = output_linear_layer(final_norm_output) # (B, T, vocab_size)\n",
    "    \n",
    "    # --- 3. Calculate Loss --- \n",
    "    B_loss, T_loss, V_loss = logits.shape\n",
    "    logits_for_loss = logits.view(B_loss * T_loss, V_loss) \n",
    "    targets_for_loss = yb.view(B_loss * T_loss)\n",
    "    loss = criterion(logits_for_loss, targets_for_loss)\n",
    "    \n",
    "    # --- 4. Zero Gradients --- \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- 5. Backward Pass --- \n",
    "    loss.backward()\n",
    "    \n",
    "    # --- 6. Update Parameters --- \n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging --- \n",
    "    current_loss = loss.item()\n",
    "    losses.append(current_loss)\n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "print(\"--- Training Loop Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Text Generation (Inline)\n",
    "\n",
    "**Goal:** Use the trained model parameters to generate new text, character by character, starting from a seed context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Set Generation Seed and Parameters\n",
    "\n",
    "**Explanation:** Define the starting character(s) for generation and how many characters to generate. We convert the seed character ('t' in this case) to its token ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Text Generation ---\n",
      "Initial seed sequence: 't' -> [[31]]\n",
      "Generating 200 new tokens...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 5: Text Generation ---\")\n",
    "\n",
    "# Seed character(s)\n",
    "seed_chars = \"t\"\n",
    "# Convert seed characters to token IDs\n",
    "seed_ids = [char_to_int[ch] for ch in seed_chars]\n",
    "\n",
    "# Create the initial context tensor\n",
    "# Shape: (1, len(seed_ids)) -> Batch dimension = 1\n",
    "generated_sequence = torch.tensor([seed_ids], dtype=torch.long, device=device)\n",
    "print(f\"Initial seed sequence: '{seed_chars}' -> {generated_sequence.tolist()}\")\n",
    "\n",
    "# Define how many new tokens (characters) to generate\n",
    "num_tokens_to_generate = 200 \n",
    "print(f\"Generating {num_tokens_to_generate} new tokens...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Generation Loop\n",
    "\n",
    "**Explanation:** Iterate `num_tokens_to_generate` times. In each iteration:\n",
    "1. Prepare the current context (last `block_size` tokens).\n",
    "2. Perform a forward pass using the *trained* model parameters (in evaluation mode - `torch.no_grad()` is used to disable gradient calculation for efficiency).\n",
    "3. Get the logits for the *last* time step.\n",
    "4. Apply softmax to get probabilities.\n",
    "5. Sample the next token ID based on probabilities.\n",
    "6. Append the new token ID to the `generated_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Set layers to evaluation mode (important if dropout/batchnorm were used)\n",
    "# This disables dropout. We do it manually since no nn.Module class.\n",
    "for i in range(n_layers):\n",
    "    layer_norms_1[i].eval()\n",
    "    mha_qkv_linears[i].eval()\n",
    "    mha_output_linears[i].eval()\n",
    "    layer_norms_2[i].eval()\n",
    "    ffn_linear_1[i].eval()\n",
    "    ffn_linear_2[i].eval()\n",
    "final_layer_norm.eval()\n",
    "output_linear_layer.eval()\n",
    "token_embedding_table.eval()\n",
    "\n",
    "# Disable gradient calculations for generation\n",
    "with torch.no_grad():\n",
    "    # Loop to generate tokens one by one\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        # --- 1. Prepare Input Context --- \n",
    "        # Take the last block_size tokens as context\n",
    "        current_context = generated_sequence[:, -block_size:] # Shape: (1, min(current_len, block_size))\n",
    "        B_gen, T_gen = current_context.shape \n",
    "        C_gen = d_model\n",
    "        \n",
    "        # --- 2. Forward Pass --- \n",
    "        # Embedding + Positional Encoding\n",
    "        token_embed_gen = token_embedding_table(current_context) # (B_gen, T_gen, C_gen)\n",
    "        pos_enc_slice_gen = positional_encoding[:, :T_gen, :] \n",
    "        x_gen = token_embed_gen + pos_enc_slice_gen # (B_gen, T_gen, C_gen)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        for i in range(n_layers):\n",
    "            x_input_block_gen = x_gen\n",
    "            # Pre-LN MHA\n",
    "            x_ln1_gen = layer_norms_1[i](x_input_block_gen)\n",
    "            qkv_gen = mha_qkv_linears[i](x_ln1_gen)\n",
    "            qkv_gen = qkv_gen.view(B_gen, T_gen, n_heads, 3 * d_k).permute(0, 2, 1, 3)\n",
    "            q_gen, k_gen, v_gen = qkv_gen.chunk(3, dim=-1)\n",
    "            attn_scores_gen = (q_gen @ k_gen.transpose(-2, -1)) * (d_k ** -0.5)\n",
    "            # Use the pre-computed mask sliced to the current context length T_gen\n",
    "            attn_scores_masked_gen = attn_scores_gen.masked_fill(causal_mask[:,:,:T_gen,:T_gen] == 0, float('-inf'))\n",
    "            attention_weights_gen = F.softmax(attn_scores_masked_gen, dim=-1)\n",
    "            attn_output_gen = attention_weights_gen @ v_gen\n",
    "            attn_output_gen = attn_output_gen.permute(0, 2, 1, 3).contiguous().view(B_gen, T_gen, C_gen)\n",
    "            mha_result_gen = mha_output_linears[i](attn_output_gen)\n",
    "            x_gen = x_input_block_gen + mha_result_gen # Residual 1\n",
    "            # Pre-LN FFN\n",
    "            x_input_ffn_gen = x_gen\n",
    "            x_ln2_gen = layer_norms_2[i](x_input_ffn_gen)\n",
    "            ffn_hidden_gen = ffn_linear_1[i](x_ln2_gen)\n",
    "            ffn_activated_gen = F.relu(ffn_hidden_gen)\n",
    "            ffn_output_gen = ffn_linear_2[i](ffn_activated_gen)\n",
    "            x_gen = x_input_ffn_gen + ffn_output_gen # Residual 2\n",
    "            \n",
    "        # Final Layers\n",
    "        final_norm_output_gen = final_layer_norm(x_gen)\n",
    "        logits_gen = output_linear_layer(final_norm_output_gen) # (B_gen, T_gen, vocab_size)\n",
    "        \n",
    "        # --- 3. Get Logits for Last Time Step --- \n",
    "        logits_last_token = logits_gen[:, -1, :] # Shape: (B_gen, vocab_size)\n",
    "        \n",
    "        # --- 4. Apply Softmax --- \n",
    "        probs = F.softmax(logits_last_token, dim=-1) # Shape: (B_gen, vocab_size)\n",
    "        \n",
    "        # --- 5. Sample Next Token --- \n",
    "        next_token = torch.multinomial(probs, num_samples=1) # Shape: (B_gen, 1)\n",
    "        \n",
    "        # --- 6. Append Sampled Token --- \n",
    "        generated_sequence = torch.cat((generated_sequence, next_token), dim=1)\n",
    "\n",
    "print(\"\\n--- Generation Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.3: Decode Generated Sequence\n",
    "\n",
    "**Explanation:** Convert the sequence of generated token IDs back into human-readable characters using the `int_to_char` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Generated Text (including seed):\n",
      "the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "So she was considerinf her wad fe f\n"
     ]
    }
   ],
   "source": [
    "# Get the generated sequence for the first (and only) batch item\n",
    "final_generated_ids = generated_sequence[0].tolist()\n",
    "\n",
    "# Decode the list of IDs back into a string\n",
    "decoded_text = ''.join([int_to_char[id] for id in final_generated_ids])\n",
    "\n",
    "print(f\"\\nFinal Generated Text (including seed):\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save the model state (optional)\n",
    "\n",
    "Since our transformer model is implemented \"inline\" with separate component variables rather than as a PyTorch nn.Module, we need to manually collect all parameters into a state dictionary before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to 'saved_models/transformer_model.pt'\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store the model (if it doesn't exist)\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Create a state dictionary to hold all model parameters\n",
    "state_dict = {\n",
    "    'token_embedding_table': token_embedding_table.state_dict(),\n",
    "    'positional_encoding': positional_encoding,  # This is not a parameter, just a tensor\n",
    "    'layer_norms_1': [ln.state_dict() for ln in layer_norms_1],\n",
    "    'mha_qkv_linears': [linear.state_dict() for linear in mha_qkv_linears],\n",
    "    'mha_output_linears': [linear.state_dict() for linear in mha_output_linears],\n",
    "    'layer_norms_2': [ln.state_dict() for ln in layer_norms_2],\n",
    "    'ffn_linear_1': [linear.state_dict() for linear in ffn_linear_1],\n",
    "    'ffn_linear_2': [linear.state_dict() for linear in ffn_linear_2],\n",
    "    'final_layer_norm': final_layer_norm.state_dict(),\n",
    "    'output_linear_layer': output_linear_layer.state_dict(),\n",
    "    # Save hyperparameters for model reconstruction\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'd_ff': d_ff,\n",
    "        'block_size': block_size\n",
    "    },\n",
    "    # Save tokenizer info for text generation\n",
    "    'tokenizer': {\n",
    "        'char_to_int': char_to_int,\n",
    "        'int_to_char': int_to_char\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the state dictionary\n",
    "torch.save(state_dict, 'saved_models/transformer_model.pt')\n",
    "print(\"Model saved successfully to 'saved_models/transformer_model.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model later, you would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved state dictionary\n",
    "loaded_state_dict = torch.load('saved_models/transformer_model.pt', map_location=device)\n",
    "\n",
    "# Extract configuration and tokenizer info\n",
    "config = loaded_state_dict['config']\n",
    "vocab_size = config['vocab_size']\n",
    "d_model = config['d_model']\n",
    "n_heads = config['n_heads']\n",
    "n_layers = config['n_layers']\n",
    "d_ff = config['d_ff']\n",
    "block_size = config['block_size']\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "char_to_int = loaded_state_dict['tokenizer']['char_to_int']\n",
    "int_to_char = loaded_state_dict['tokenizer']['int_to_char']\n",
    "\n",
    "# Recreate the model components\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "token_embedding_table.load_state_dict(loaded_state_dict['token_embedding_table'])\n",
    "\n",
    "positional_encoding = loaded_state_dict['positional_encoding'].to(device)\n",
    "\n",
    "# Initialize the layer lists\n",
    "layer_norms_1 = []\n",
    "mha_qkv_linears = []\n",
    "mha_output_linears = []\n",
    "layer_norms_2 = []\n",
    "ffn_linear_1 = []\n",
    "ffn_linear_2 = []\n",
    "\n",
    "# Load each layer's components\n",
    "for i in range(n_layers):\n",
    "    # Layer norm 1\n",
    "    ln1 = nn.LayerNorm(d_model).to(device)\n",
    "    ln1.load_state_dict(loaded_state_dict['layer_norms_1'][i])\n",
    "    layer_norms_1.append(ln1)\n",
    "    \n",
    "    # MHA QKV linear\n",
    "    qkv_linear = nn.Linear(d_model, 3 * d_model, bias=False).to(device)\n",
    "    qkv_linear.load_state_dict(loaded_state_dict['mha_qkv_linears'][i])\n",
    "    mha_qkv_linears.append(qkv_linear)\n",
    "    \n",
    "    # MHA output linear\n",
    "    output_linear = nn.Linear(d_model, d_model).to(device)\n",
    "    output_linear.load_state_dict(loaded_state_dict['mha_output_linears'][i])\n",
    "    mha_output_linears.append(output_linear)\n",
    "    \n",
    "    # Layer norm 2\n",
    "    ln2 = nn.LayerNorm(d_model).to(device)\n",
    "    ln2.load_state_dict(loaded_state_dict['layer_norms_2'][i])\n",
    "    layer_norms_2.append(ln2)\n",
    "    \n",
    "    # FFN linear 1\n",
    "    lin1 = nn.Linear(d_model, d_ff).to(device)\n",
    "    lin1.load_state_dict(loaded_state_dict['ffn_linear_1'][i])\n",
    "    ffn_linear_1.append(lin1)\n",
    "    \n",
    "    # FFN linear 2\n",
    "    lin2 = nn.Linear(d_ff, d_model).to(device)\n",
    "    lin2.load_state_dict(loaded_state_dict['ffn_linear_2'][i])\n",
    "    ffn_linear_2.append(lin2)\n",
    "\n",
    "# Final layer norm\n",
    "final_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "final_layer_norm.load_state_dict(loaded_state_dict['final_layer_norm'])\n",
    "\n",
    "# Output linear layer\n",
    "output_linear_layer = nn.Linear(d_model, vocab_size).to(device)\n",
    "output_linear_layer.load_state_dict(loaded_state_dict['output_linear_layer'])\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Conclusion\n",
    "\n",
    "This notebook provided an extremely detailed, step-by-step, inline implementation of a character-level Decoder-only Transformer Language Model. By avoiding functions and classes, we exposed the granular operations involved in both training and text generation.\n",
    "\n",
    "We covered:\n",
    "1.  **Setup & Tokenization:** Preparing the environment, defining a text corpus, performing character-level tokenization (creating character mappings and encoding the corpus), and setting hyperparameters.\n",
    "2.  **Data Preparation:** Structuring the encoded corpus into input/target pairs for next-token prediction.\n",
    "3.  **Model Initialization:** Creating instances of all necessary `torch.nn` layers (Embeddings, Linears, LayerNorms) and precomputing Positional Encodings.\n",
    "4.  **Forward Pass (Inline):** Detailing and executing the flow through embeddings, positional encoding, multiple Transformer blocks (with Masked Multi-Head Self-Attention and Feed-Forward networks, including residual connections and Layer Normalization - using a Pre-LN structure), and the final output layers.\n",
    "5.  **Training:** Implementing the training loop with batch sampling, forward pass execution, Cross-Entropy Loss calculation, backpropagation, and parameter updates via the AdamW optimizer.\n",
    "6.  **Text Generation:** Demonstrating autoregressive generation by starting with a seed, iteratively performing forward passes on the growing context (within `torch.no_grad()`), sampling the next token based on output probabilities, and appending it to the sequence, finally decoding the generated IDs back to text.\n",
    "\n",
    "While highly verbose, this approach clearly illustrates the fundamental mechanics and data flow within a Transformer LM. Real-world implementations would heavily utilize functions and classes for modularity, reusability, and readability, but this inline method serves as a detailed educational breakdown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-multimodaal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
